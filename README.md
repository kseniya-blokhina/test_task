# Overview of standards and UTF-8 & UTF-16 encodings 

## Historical background

<p align="justify"> While working with different types of information such as text strings, images or videos, computers deal with information presented in a digital form, and therefore the storage of information including text lines and individual graphic symbols proceeds in a digital form. The tool that allows you to describe the rules for storing text and the correspondence of the digital code of a symbol to its graphic image is the encoding.</p>

<p align="justify"> In the early days of the personal computer, 7-bit code pages were used to store values in memory with a single graphic symbol corresponding to each one. Such code pages could contain up to 128 different characters, and this was enough for all letters of the English alphabet, as well as basic mathematical symbols and punctuation marks. The main encoding and the corresponding code page were named ASCII (American Standard Code for Information Interchange). Then, the spread of computer technology around the world resulted in a grown up demand for encoding of various national language characters. The original ASCII code was used as the base one and occupied the first 7 bits, and the 8th was added to the national language, so all the necessary national characters were encoded with the most significant bit and occupied the remaining 128 positions while maintaining the support of the English language. Due to the growing number of different encodings and corresponding code pages, manufacturers of operating systems and software began to have difficulties with technical support and updating their product to ensure compatibility.</p>

## Features of the new standard

<p align="justify"> The chaos associated with the lack of common standards forced the creation of a universal encoding that can contain all the specific characters at once and take into account the features of the writing of different nations of the world. This aim was achieved by creating a single standard that met these requirements, containing also an algorithm for determining encodings. In 1991 Unicode Inc. in collaboration with ISO, began developing the Unicode and ISO / IEC 10646 standards, respectively. Subsequently, the updating of these standards also took place approximately simultaneously. One of the main principles of Unicode is the idea that each character is understood as some kind of abstraction, i.e. regardless of how the characters are stored in computer memory or displayed on an input and output device, the character will remain the same. This idea is well known in programming as encapsulation or separation of interface from implementation.</p>

<p align="justify"> According to the new standard, it was decided to dedicate 2<sup>16</sup> (65 535) code positions for a set of national symbols, which was named Basic Multilingual Plane (BMP). At first, the UCS (Universal Character Set) encoding was developed according to the ISO 10646 standard in the UCS-2 and UCS-4 standards with a fixed code length of 16 and 32 bits, respectively. The fixed multiple of two code length and a simple coding method on the one hand ensured high speed of text processing, but on the other hand such code occupied too much space. </p>

<p align="justify"> Then began the development of the UTF (Unicode Transformation Format) variable-length encodings, where the number of bytes occupied by a character depends on its location in the code table. The UTF-16 encoding was created based on UCS-2, wherein the code points were outside the BMP and each character was written in 2 or 4 bytes (surrogate pairs) in the range from 0 to FFFF<sub>16</sub> with an increased number of code points to 2<sup>20</sup>+2<sup>16</sup>âˆ’2048 (1 112 064). Seeing as most communication and storage protocols are defined for bytes with each unit taking two 8-bit bytes. The order of the bytes may depend on the endianness of the computer architecture. Decreasing and increasing numeric significances of bytes are known as big-endian and little-endian, respectively. Later it was provided the UTF-8 encoding, wherein each character could be written in 1, 2, 3 or 4 bytes, and the number of code points was increased to 2<sup>20</sup> (2 097 152). However, for compatibility with UTF-16 the same number of code points has remained. Unlike UTF-16, UTF-8 is independent of byte order. </p>

<p align="justify"> Today there are 17 planes that can accommodate 1,114,112 code points. They are identified by the numbers 0 to 16 with possible values from 00 to 10<sub>16</sub> of the first two positions in six position hexadecimal format (U+hhhhhh). The Basic Multilingual Plane (BMP) is defined under the 0 plane. The other planes are called "supplementary planes" with planes 4-13 (40000-DFFFF) remaining unused. Hence it appears to be no lack of code points now and both UTF-8 and UTF-16 meet present requirements.</p>

## Comparison of UTF-8 and UTF-16

<p align="justify"> In order to understand which of these encodings is better to use when creating your product, a brief comparative analysis of these two encodings should be done. Both are variable-length encodings, and both have a maximum character size of 4 bytes. </p>

<p align="justify"> In almost all cases, UTF-8 is better, as it is still the most thoughtful and commonly used variable-length encoding at the moment. The most frequently used characters, such as syntactic, mathematical signs and letters of the Latin alphabet, as stated above, are written in 1 byte in both ASCII and UTF-8. Thus, all the ASCII strings are valid in UTF-8, which provides a decent backwards compatibility in many cases. There are no null bytes, which allows null-terminated strings, this introduces more backward compatibility. But many common characters have different lengths, which slows down indexing and calculating the length of the string. Anyway, at the moment, there is a trend of completely switching to UTF-8 encoding, because it turned out to be the most convenient for the network protocol.</p>

<p align="justify"> However, in the case of using a text editor, for example, to store and process East Asian scripts, it may be better to use UTF-16 encoding, since most characters are 2 bytes here, while UTF-8 is usually 3 bytes. Also, 2 bytes are occupied by Latin and Cyrillic characters. If it is not intended to use unusual characters that take up 4 bytes, then it is possible to conventionally perceive the UTF-16 encoding as a fixed-width encoding, which will speed up indexing and provide a better representation in memory. But there are lots of null bytes in ASCII strings, which means there are no null-terminated strings and lots of wasted memory. In addition, one of the uses of text editors is code review and quick edition, and some applications such as Java String, C # String, Win32 API, Qt GU libraries continue to use UTF-16 encoding. However, to be honest, the idea that we need to use more UTF-16 is an unpopular opinion.</p>
